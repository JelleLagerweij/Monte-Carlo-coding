\section{Setting up the code}
In the following section, some physics needed for the Monte Carlo code is explained, followed by the code used to implement these phenomena. Before these functions are shown, it should be mentioned that I used object oriented coding for the assignment: I use a class to keep track of the state of my ensemble. Therefore, some functions will be implemented slightly different than instructed in the assignment. To show the functions more compact in the report, I decided to remove the comments in this report. The full code, including the comments can be found on my GitHub page \footnote{\href{https://github.com/JelleLagerweij/Canonecal-Monte-Carlo-for-Lennard-Jones-fluids}{Link to GitHub page of this project}}.
\subsection{Total energy}
To calculate the total energy of the ensemble in a certain configuration can be done by computing the following summation:

\begin{equation}
	U_\text{tot} = \begin{cases}
		4\varepsilon\sum_{i=0}^{N}\sum_{j>i}^{N}\left[\left(\frac{\sigma}{r}\right)^{12}-\left(\frac{\sigma}{r}\right)^6\right] &\quad r_{ij}\leq r_\text{cut}\\
		0&\quad r_{ij}>r_\text{cut}
	\end{cases} 
\end{equation}

Where the tail and shift correction can be implemented using the following equations:

\begin{align}
	U_\text{shift}&=4\varepsilon\left[\left(\frac{\sigma}{r_\text{cut}}\right)^{12}-\left(\frac{\sigma}{r_\text{cut}}\right)^6\right]\\
	U_\text{tail}&=\frac{8}{3}\pi\rho_\text{N}\varepsilon\sigma^3\left[\frac{1}{3}\left(\frac{\sigma}{r_c}\right)^9-\left(\frac{\sigma}{r_c}\right)^3\right]
\end{align}

For correct computations of this, all $r_{ij}$ should be calculated first. As these distances are used for the virial pressure as well, computing them is done in a separated function.

\begin{listing}[ht!]
	\begin{minted}{python}
def allDistances(self):
	x = self.x
	L = self.L
	N, D = x.shape
	
	r = np.broadcast_to(x, (N, N, D))
	rel_r = (r - r.transpose(1, 0, 2) + L/2) % L - L/2
	d_sq = np.einsum('ijk, ijk->ij', rel_r, rel_r, optimize='optimal')
	np.fill_diagonal(d_sq, np.inf)
	self.d_sq = d_sq 
	\end{minted}
\caption{The function which calculates all distances between all particles.}
\label{Distances function}
\end{listing}

This is a function included in the state, therefore, the only input needed is \mintinline{python}{self}. Of the state recorded in the class, only the locations of all current particles and the size of the box are needed to compute all distances. I decided to optimise the distance computations. In line 7, the size of the position array is increased to a 3D array, by copying the positional information N times. Subtracting the transpose of this from itself computes the inter particle distances immediately. Using the modulus statement applies the periodic boundary conditions. From these, the square of the norm of these position vectors are calculated. The \mintinline{python}{numpy.broadcast()} and \mintinline{python}{numpy.einsum()} are chosen for computational efficiency. The einsum is tested to be faster than \mintinline{python}{numpy.linalg.norm()} and \mintinline{python}{numpy.broadcast()} creates the larger array, but changes the memory access instead of creating the full array in the memory.

Now the total energy can be computed. This function needs the imputs $\sigma$, $\varepsilon$, $r_\text{cut}$ and more importantly all the distances computed. Besides that, the shift ant tail corrections (if activated in the code) are included as well.It must be noted that the distance function only computes the squared distances, as there is no use in computing the root anyway. The code calculates the Van der Waals and Pauli parts separately and then calculates the total energy from combining them. To include the correct tail and shift corrections, the amount of particles within the cut off distance and outside the cut off distance is evaluated.

\begin{listing}[ht!]
	\begin{minted}{python}
def totalEnergy(self):
    d_sq = np.copy(self.d_sq)
	N = d_sq.shape[0]
	Rcut = self.Rcut
	eps = self.eps
	sig = self.sig
	tail = self.tail
	shift = self.shift
	
	d_sq[d_sq > Rcut**2] = np.inf
	sr6 = sig**6/(d_sq*d_sq*d_sq)
	sr12 = sr6*sr6
	n = np.count_nonzero(sr6) 
	Etot = 2*eps*np.sum(sr12-sr6) - n*shift/2 + N*tail
	self.Etot = Etot
	return Etot
	\end{minted}
\caption{The total energy function.}
	\label{Total energy function}
\end{listing}


\subsection{Single particle energy}
For the single particle energy, a similar approach is taken. However, as the only use for the distances between one single particle and all the others is that of the single particle energy, the decision is made to combine them into one single function. The inputs for this functions are the current state of the system: The current positions of all particles, the box size, and the constants: $\varepsilon$, $\sigma$, and the cut-off distance $r_\text{cut}$. The only manual input needed is the index of the particle of which the interaction energy is investigated.

\begin{listing}[ht!]
	\begin{minted}{python}
def singleEnergy(self, Ni, trial=False):
	sig = self.sig
	eps = self.eps
	tail = self.tail
	shift = self.shift
	
	if trial is True:
		x = self.x_trial
	else:
		x = self.x
	
	L = self.L
	Rcut = self.Rcut
	N, D = x.shape
	
	rel_r = (x - x[Ni, :] + L/2) % L - L/2
	d_sq = np.einsum('ij, ij -> i', rel_r, rel_r)
	d_sq[Ni] = np.inf
	d_sq[d_sq > Rcut**2] = np.inf 
	
	sr_2 = sig**2/d_sq
	sr6 = sr_2*sr_2*sr_2
	sr12 = sr6*sr6
	
	n = np.count_nonzero(sr6)
	E_single = 4*eps*np.sum(sr12 - sr6) - n*shift + tail
	return E_single
	\end{minted}
\caption{The single particle energy function. Note that the two different possitional states can be evaluated, the normal position, \mintinline{python}{self.x}, and optionally the trial position list \mintinline{python}{self.x_trial}.}
	\label{Single energy function}
\end{listing}

\subsection{Virial pressure}
The virial pressure can be computed using the same particle distances as derived using \cref{Distances function}. Besides that, the pressure resulting from these interactions can be computed. For the pressure the current state is needed, including the computed list of particle distances. Just like the energy functions, from this state, the constants $\varepsilon$ and $\sigma$ are needed. The number density can be computed from the box size and the list with particle distances. The distances that are used already satisfy the minimal image convention and self interactions are removed. As there is no correction option included for the virial pressure equation, the decision is made to not cut off the particle interactions. In python, the code for the virial pressure is the following:

\begin{listing}[ht!]
	\begin{minted}{python}
def pressure(self):
	T = self.T
	sig = self.sig
	eps = self.eps
	L = self.L
	V = L**3
	d_sq = self.d_sq
	sr2 = sig**2/d_sq
	sr6 = sr2*sr2*sr2
	sr12 = sr6*sr6
	P = co.k*T*d_sq.shape[0]/V - 24*eps*np.sum(sr6 - 2*sr12)/(6*V)
	self.P = P
	return P
	\end{minted}
\caption{The virial pressure function.}
	\label{Virial pressure function}
\end{listing}

It must be noted that the pressure calculation and the total energy computation both use summations over the Van der Waals and Pauli interaction. Computational efficiency could be increased by creating a single function which computes the $\left(\frac{\sigma}{r_{ij}}\right)^6$ and $\left(\frac{\sigma}{r_{ij}}\right)^{12}$ for both the total energy and pressure. However, then the functions of the observables would mix up a little, decreasing the readability of the code.

\subsection{Translation moves}
For the Monte Carlo simulation, every iteration exist of a trial move, which is than accepted or rejected by assessing the change in energy. For this, the current state of the system is used. Besides the particle positions, the temperature is needed to evaluate the acceptance probabilities. The translation move function performs a relatively simple task: It chooses a random particle, applies a random displacement after which the energy of this single particle is tested in the original configuration and the trial configuration. The trial configuration is accepted using the following probability criterion:
\begin{equation}
	\text{acc}(\text{old} \rightarrow \text{trial})=\text{min}\left(1,\: e^{-\frac{U_\text{trial}-U_\text{old}}{k_\text{B}T}} \right)
\end{equation}
In other words, if the energy of the trial configuration is lower than that of the original configuration, the trial state will be accepted. If the trial energy is higher than that of the original configuration the acceptance probability decreases by Boltzmann statistics. This probability decreases the larger $U_\text{trial}$ is compared to $u_\text{old}$.

\begin{listing}[ht!]
	\begin{minted}{python}
def translate(state):
	x = state.x
	N, D = x.shape
	T = state.T
	L = state.L
	max_step = state.max_step

	Ni = np.random.randint(0, high=N)
	trial_move = 2*max_step*np.random.rand(1, 3) - 1*max_step
	x_trial = np.copy(x)
	x_trial[Ni, :] = (x_trial[Ni, :] + trial_move) % L
	state.x_trial = x_trial

	U = state.singleEnergy(Ni)
	U_trial = state.singleEnergy(Ni, trial=True)
	dU = U_trial - U

	if dU < 0:
		state.x = state.x_trial
		state.accept = state.accept + 1
	else:
		p_acc = np.exp(-dU/(co.k*T))
		if p_acc > np.random.rand():
			state.x = state.x_trial
			state.accept = state.accept + 1
	\end{minted}
\caption{The translate function. Note that for the trial move, an explicit copy is made of the original configuration to be able to treat both configurations separately. The single particle energies for both configurations are computed using \cref{Single energy function} and the trial configuration is accepted using Boltzmann statistics.}
	\label{translate function}
\end{listing}

\subsection{Initial configuration setup}
The initial configuration for the Monte Carlo method exists of two function in my code. The first function initialises the python Class that I build and the second function places the particles in their initial position. The inputs asked are the following:
the Temperature in in \si{\K}, the density in \si{\kg\per\m^3}, the atomic mass of the particles in amu, the LJ energy constant $\varepsilon$ in units of $k_\text{B}$ and the LJ distance constant $\sigma$ in \si{\angstrom}.

\begin{listing}[ht!]
	\begin{minted}{python}
class State:
	def __init__(self, T, rho, m_a, eps, sig, N_or_file):
		self.T = T
		self.rho = rho
		self.m_a = m_a
		self.eps = eps*co.k
		self.sig = sig*1e-10
		self.tail = 0
		self.shift = 0
		self.initialConfiguration(N_or_file)
	
	def initialConfiguration(self, N_or_file):
	if type(N_or_file) is str:
		data = pd.read_csv(N_or_file, delim_whitespace=True, header=None,
		skiprows=2)
		self.x = (np.array((data[1], data[2], data[3])).T)*1e-10
		self.L = np.round(np.max(np.abs(self.x*1e10)))*1e-10
		self.Rcut = self.L/2
		self.rho = self.m_a*self.x.shape[0]/(1000*co.N_A*self.L**3)
	
	elif type(N_or_file) is int:
		self.L = np.power(self.m_a*N_or_file/(co.N_A*1000*self.rho), 1/3)
		self.Rcut = self.L/2
		self.x = np.random.rand(N_or_file, 3)*self.L
	
	else:
		raise ValueError("N_or_file has to be the ammount of particles",
		"int, or the file location with an initial state")
	\end{minted}
\caption{These functions initialise the class.}
	\label{Initialisation code}
\end{listing}

\begin{listing}[ht!]
	\begin{minted}{python}
		def modelCorrections(self, Rcut=False, Tail=False, Shift=False):
		sig = self.sig
		eps = self.eps
		
		if Rcut is not False:
		Rcut = Rcut*1e-10
		if Rcut > self.L/2:
		print('Cutoff distance is more than half the box size, as',
		'remidy, Rcut is set to default, Rcut = L/2')
		else:
		self.Rcut = Rcut
		
		Rcut = self.Rcut
		if Tail is not False:
		tail_factor = (8*np.pi*N*eps*np.power(sig, 3))/(3*np.power(L, 3))
		tail_distances = (np.power(sig/Rcut, 9)/3) - np.power(sig/Rcut, 3)
		self.tail = tail_factor*tail_distances
		
		if Shift is not False:
		shift_factor = 4*eps
		shift_distances = np.power(sig/Rcut, 12) - np.power(sig/Rcut, 6)
		self.shift = shift_factor*shift_distances
	\end{minted}
\caption{The optional code to call the model correction terms. These correction terms are all collected into one function, however can be set to true or false individually.}
	\label{Model correction code}
\end{listing}

The special input is the option \mintinline{python}{N_or_file}, which needs either the file location with the initial particle positions or the number of particles needed to model. This parameter is then passed to another function, the initial configuration function. Here, two options exists:
\begin{enumerate}
	\item \mintinline{python}{N_or_file} is of string type. In that case the file location corresponding to the string is read and the particle positions are copied from these positions. The box size is then estimated from the particle positions and the density is recalculated as well. Besides that, the cut-off distance is set to half the box size by default.
	\item \mintinline{python}{N_or_file} is of type integer. This results in calculating the box size for the inputted density and atomic mass and the amount of asked for particles. After that, $N$ particles are placed in random positions within this box size. $r_\text{cut}$ is again set to the default $L/2$. 
\end{enumerate}

Besides these initialisation functions, a last function exists where optional model changes are made. This function can be used to change the default cut-off distance as well as change the default values for the shift and tail corrections. These are, by default, set to $0$, however can be set to correctly represent a truncated Lennard-Jones potential.

\subsection{Averaging for the results}
Using the ergodicity postulate, the average of the observables over all configurations approaches the (true) time average of these observables. Therefore the observables are averaged over the configurations visited by the Monte Carlo algorithm. As only undergoing one trial move will not change the observables by much and computing the observables takes considerable computational effort, these observables are only collected every $2N$ trial moves. Still, some autocorrelation may still exist in the collected data. Therefore, using the standard deviation of the data is not the correct way to determine the uncertainty in the result. To have a better expression for the error in the collected data, firstly, the autocorrelation is computed and fitted to an exponential function with a unknown decay factor $\tau$. The estimated uncertainty of the averaged value is then computed using the following equation;

\begin{equation}
	u_\text{uncertainty} = \sqrt{2\tau \frac{ \sum_i(u_i - \langle u\rangle)^2}{n}}
\end{equation}

where $u_i$ are all calculated values of an observable and $\langle u\rangle$ is the average of these values. $n$ represents the total number of samples and $\tau$ the correlation decay factor. The code needed for the correct averaging and estimating the is presented below.

\begin{listing}[ht!]
	\begin{minted}{python}
def statistics(s):
	N = s.shape[0]
	mean = s.mean()
	var = np.var(s)

	if var == 0.0:
		mean, error, tao, g = mean, 0, 0, 0

	else:
		sp = s - mean
		corr = np.zeros(N)
		corr[0] = 1
		for n in range(1, N):
			corr[n] = np.sum(sp[n:]*sp[:-n]/(var*N))
		g = np.argmax(corr < 0.1)
		t = np.arange(2*g)
		tao = opt.curve_fit(lambda t, b: np.exp(-t/b),  t,
		corr[:2*g], p0=(g))[0][0]
		error = np.sqrt(2*tao*s.var()/N)
	return (mean, error)
	\end{minted}
	\caption{The code that averages the observables and computes their uncertainty while adjusting for autocorrelation.}
	\label{Statistical assesment}
\end{listing}

\subsection{The Monte Carlo function, linking everything together}
All the pieces of code are connected together by the Monte Carlo function. This function has the following input; the state used (which has to be the initiated class), the amount of times that the observables are sampled, the initial maximum step size and an optional setting to turn on or of the equilibrating of the initial configuration. Between the computing of consecutive observables, $2N$ trial steps are performed. The max step size for the trial move is also adjusted every $2N$ trials. This function is explained in \cref{max step size}. The current version of the Monte Carlo algorithm tracks three observables, the total energy in the system, the pressure of the system and the radial distribution function. The Monte Carlo function itself is presented below:

\begin{listing}[ht!]
	\begin{minted}{python}
def monteCarlo(state, n, max_step_init, startup_eq=True):
	state.max_step = max_step_init*1e-10
	x = state.x
	L = state.L
	N, D = x.shape
	
	if startup_eq is True:
		for j in range(100):
		state.accept = 0
		for i in range(N):
			translate(state)
		state.acceptance = state.accept/(N)
		state.newStepsize()
	
	# Now real measurement data is generated
	E_tot = np.zeros(n)
	P = np.zeros(n)
	rad_dis = np.zeros((1000, n))
	state.allDistances()
	r = np.histogram(np.sqrt(state.d_sq), bins=1000, range=(0, L/2))[1]
	r = r[:-1] + (r[1]-r[0])/2  # remove last bin edge and shift to center bins
	
	for i in range(n):
		state.accept = 0
		for j in range(2*N):
			translate(state)
		state.acceptance = state.accept/(2*N)
		state.newStepsize()
		state.allDistances()
		E_tot[i] = state.totalEnergy()
		P[i] = state.pressure()
		n_r = np.histogram(np.sqrt(state.d_sq), bins=1000, range=(0, L/2))[0]
		rad_dis[:, i] = (L**3*n_r)/(N*(N-1)*4*np.pi*(r**2)*(r[1]-r[0]))
		state.progress = i/n
		update_progress(state.progress)
	
	state.progress = 1
	update_progress(state.progress)
	
	state.trial_moves = 2*N*n
	state.E_tot = E_tot
	state.P = P
	state.rad_dis = rad_dis
	state.r_bins = r
	
	E_tot = statistics(E_tot)
	P = statistics(P)
	rad_dis_m = np.zeros((rad_dis.shape[0], 2))
	for i in range(rad_dis.shape[0]):
		rad_dis_m[i, :] = statistics(rad_dis[i, :])
	return E_tot, P, rad_dis_m, r
	\end{minted}
\caption{The Monte Carlo function. The logic is as following, first an optional MC process to reach equilibrium is performed, after which the observables are introduced as empty arrays and then the MC algorithm is executed while storing the observables every $2N$ trial moves. After the MC process had run, the arrays containing the observables are stored in the class, but he average results and their uncertainties are returned immediately.}
	\label{Monte Carlo}
\end{listing}
